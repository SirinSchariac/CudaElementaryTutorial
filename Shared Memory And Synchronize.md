## 共享内存与同步
通过`__shared__`关键字将指定的变量驻留在共享内存中，每个线程块都拥有共享内存的私有副本，而线程无法访问或修改其他线程块的变量副本，因此，同一个线程块上的线程之间可以进行通信和协作。共享缓存具有更低的访问延迟，但也带来了竞态问题(Race Condition)，因此需要进行同步(synchronize)。

该部分的编码在Dot Product里。

对于共享变量进行操作时，需要保证所有线程对于该共享变量的写入工作都已经完成，同步工作通过以下函数实现。
```
__syncthreads();
```
该函数确保了线程块中的所有线程都执行完了`__syncthreads()`前面的语句之后，才会执行后续的语句。

> **规约运算(Reduction)**
> 对输入数组进行一定的运算操作，产生一个更小的结果数组

一般来说，大规模的规约可以使用GPU来操作，而最后小规模的规约应当返回到CPU上执行，避免对GPU计算资源的浪费。

在GPU上对于cache的规约中，使用的代码为：
```
int i = blockDim.x/2;
while(i != 0)
{
    if(cacheIdx < i)
    {
        cache[cacheIdx] += cache[cacheIdx + i];
    }
    __syncthreads();
    i /= 2;
}
```
这里采用的是一种类似二分的思想来进行规约，即每次将cache分为两半，逐步相加，以对数级的复杂度完成了规约求和。
> **Q:能否把`__syncthreads()`移动到`if`语句块里?**
> 这样似乎很合理，因为同步实际上只需要面向那些更新了cache内容的线程，使得下一次迭代前所有的更新已经完成，不需要等待没有更新过的线程。
> 但实际上，这样做会导致GPU停止响应。这是因为，对于条件执行的语句，会出现线程发散(Threads Divergence)的情况，在正常环境中，发散的分支会让一些线程处于空闲状态，而其他的线程则会执行分支中的代码。
> 但是在启用`__syncthreads()`的情况下，线程发散会导致严重的问题。CUDA架构会保证，**除非线程块中的所有线程都执行了`__syncthreads()`，否则没有线程可以执行后续的语句。**因此，在上述的例子中，如果把`__synthreads()`移入`if`语句块中，就会导致一些线程无法执行该语句，进而导致所有线程都被挂起。
